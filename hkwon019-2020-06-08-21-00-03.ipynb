{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3, due FRIDAY, JUNE 5, 2020 @ 11:59pm\n",
    "\n",
    "This problem set covers logistic regression, cross validation, and kernels.\n",
    "\n",
    "*Please*:\n",
    "1. Complete the information below with your name, e-mail address, and student ID number\n",
    "2. Complete the assignment carefully, working through all parts\n",
    "3. Before submitting, select \"Restart Kernel and Run All Cells...\" from the \"Kernel\" menu, to make sure this notebook runs cleanly.\n",
    "4. Submit using the \"submitps\" command\n",
    "\n",
    "<b>Your information</b>:<br>\n",
    "Name: Hyuna Kwon <br>\n",
    "e-mail address: hkwon019@ucr.edu <br>\n",
    "SID #: 862063261 <br>\n",
    "\n",
    "By submitting this notebook, you are asserting that the work presented is your own, was completed without external aid, and was completed for this offering of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set, we will try bagging and boosting with decision trees as the base classifiers.\n",
    "\n",
    "For all plots:\n",
    "- Plot the error rate (vertical axis) versus the number of trees (horizontal axis)\n",
    "- plot all data on a semi-log plot (# of trees being in log)\n",
    "- try to avoid plotting for # of trees=2, as this is a weird case (hard to vote with just 2 base classifiers)\n",
    "- Draw **testing** error with solid lines\n",
    "- Draw **training** error with dashed lines\n",
    "- Draw errors for the same method with the same color (differentiating by solid/dashed, as above)\n",
    "- Use the same horizontal and vertical axis limits for **all** plots\n",
    "- label axes and title your plots\n",
    "- use a legend to distinguish the different lines on the plot\n",
    "\n",
    "For this problem set, we will treat the \"testing\" data as cross-validation data.  That is, we will assume it is available to us to judge the future performance on true testing data.\n",
    "\n",
    "First, load the data, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "trainX = np.loadtxt('spamtrainX.data')\n",
    "trainY = np.loadtxt('spamtrainY.data')[:,np.newaxis]\n",
    "mux = np.mean(trainX,axis=0)\n",
    "trainX -= mux\n",
    "stdx = np.std(trainX,axis=0)\n",
    "trainX /= stdx\n",
    "\n",
    "testX = np.loadtxt('spamtestX.data')\n",
    "testX = (testX-mux)/stdx\n",
    "testY = np.loadtxt('spamtestY.data')[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (4 points)\n",
    "\n",
    "***\n",
    "First, let's consider the VC-dimension of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (a) (2 points)\n",
    "\n",
    "What is the VC-dimension of a decision trees of depth $d$ when used on one-dimensional data ($n=1$)?\n",
    "\n",
    "Prove your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : VC-dimension of a decision trees of a depth $d$ when used on one-dimensional data is 2$^{d-1}$([log$_2$(3-d)]+1) <br>\n",
    "Proof :\n",
    "\n",
    "If d = 1 $\\rightarrow$ VC-dimension is 2. <br>\n",
    "Because $x_1 \\leq t_1$ can only shatter one or two data.\n",
    "\n",
    "If d = k+1 $\\rightarrow$ VC-dimension is\n",
    "\n",
    "VCDimension lower-bound-binary(DT) <br>\n",
    "if DT is a leaf node <br>\n",
    "    $\\quad$return 1 <br>\n",
    "if left and right subtress of DT are leaves <br>\n",
    "    $\\quad$return 2 <br>\n",
    "DT$_L$ = Left subtree of DT <br>\n",
    "DT$_R$ = Right subtree of DT <br>\n",
    "return lower-bound-binary(DT$_L$) + lower-bound-binary(DT$_R$) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (b) (2 points)\n",
    "\n",
    "Is this the same or different than the VC-dimension of a decision trees of depth $d$ when used on two-dimensional data ($n=2$)?\n",
    "\n",
    "Clearly explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: This is different from the VC-dimension of a decision trees of depth $d$ when used on two-dimensional data.\n",
    "\n",
    "When d = 2 on one-dimentional data, VC-dimension = 2\n",
    "When d = 2 on two-dimensional data, VC-dimension = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (11 points)\n",
    "\n",
    "***\n",
    "\n",
    "### Supplied learning functions:\n",
    "\n",
    "Below are three functions that will train a decision tree, and predict using that decision tree.\n",
    "They use sklearn.  **You may use these supplied functions, but not sklearn for this problem set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a decision tree to be used with treepred, below\n",
    "# samplewts can be added to provide a different weight per X row\n",
    "# (b/c this will be used with an ensemble, no pruning is done!)\n",
    "def traindt(X,Y,maxdepth,samplewts=None):\n",
    "    from sklearn.tree import DecisionTreeClassifier # do not import anything from sklearn other than this line inside this function\n",
    "    tree = DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=maxdepth)\n",
    "    tree.fit(X,Y,sample_weight=samplewts)\n",
    "    return tree\n",
    "\n",
    "# will return array m-by-2\n",
    "# each row is a different row from x\n",
    "# each column is a different class\n",
    "# value is prob of that class for that example\n",
    "def treepred(tree,x):\n",
    "    return tree.predict_proba(x)\n",
    "\n",
    "# same as treepred, but returns the difference between the\n",
    "# probabilities, which can be used as a discriminator (>0 => +1, <0 => -1)\n",
    "def treeout(tree,x):\n",
    "    f = treepred(tree,x)\n",
    "    return (f[:,1]-f[:,0])[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (a) (7 points) Bagging (2 points) and Boosting (3 points)\n",
    "\n",
    "Below, write your code to learn via bagging and boosting (two separate methods).  Your functions should take in the number of trees (rounds of bagging/boosting) and the maximum depth of the tree to be used on each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxdepth = 3\n",
    "#tree = traindt(trainX,trainY,maxdepth)\n",
    "\n",
    "\n",
    "def bagging(data,X,Y,depth,numberOfTrees):\n",
    "    bootstrapData = bootstrap(data)\n",
    "    train_x = X\n",
    "    train_y = Y\n",
    "    trees = traindt(train_x,train_y,depth)\n",
    "    return trees\n",
    "\n",
    "def bootstrap(data):\n",
    "    # The following method generates a bootstrap dataset using the input dataset\n",
    "    # It picks random incides from input data (without replacement)\n",
    "    # and uses those indices to create the bootstrap dataset of same size\n",
    "    \n",
    "    [m,d] = data.shape\n",
    "    indices = np.random.randint(m-1,size = m)\n",
    "    indices = indices.T + 1\n",
    "    bootstrapData = data[indices,:]\n",
    "    return bootstrapData\n",
    "\n",
    "def boosting(data,X,Y,depth,numberOfTrees):\n",
    "    \n",
    "    [m,d] = data.shape\n",
    "    print(m,d)\n",
    "    alpha = np.ones(m)\n",
    "    trees = np.zeros((1,numberOfTrees))\n",
    "    w = np.zeros(numberOfTrees)\n",
    "    for i in range(numberOfTrees):\n",
    "        train_x = X\n",
    "        train_y = Y\n",
    "        trees = traindt(train_x,train_y,depth,alpha)\n",
    "        y_pred = treeout(trees,train_x)\n",
    "        indices = np.argwhere(np.sign(y_pred)!=np.sign(train_y))\n",
    "        error = (sum(alpha[indices])/sum(alpha))\n",
    "        error = error[1]\n",
    "        w[i] = np.log((1-error)/error)\n",
    "        alpha[indices] = alpha[indices]*np.exp(w[i])\n",
    "    \n",
    "    return [trees,w]\n",
    "\n",
    "\n",
    "def predictBagging(x,trees,numberOfTrees):\n",
    "#This method uses the trees generated by the bagging function to\n",
    "# predict the output for input data. It uses nubmerOfTrees param to use\n",
    "# only a part of trees to predict the output.\n",
    "    y = np.zeros(x.shape[0])\n",
    "    \n",
    "    for i in range(numberOfTrees):\n",
    "        y = y + treeout(trees[i],x)\n",
    "        \n",
    "    y = y/numberOfTrees\n",
    "    y = np.where(y >= 0, 1, -1)\n",
    "    \n",
    "    return y\n",
    "    \n",
    "def predictBoosting(x,trees,w,numberOfTrees):\n",
    "    \n",
    "    y = np.zeros(x.shape[0])\n",
    "    for i in range(numberOfTrees):\n",
    "        y = y + w[i]*treepred(trees,x)\n",
    "    \n",
    "    y = np.where(y >= 0, 1, -1)\n",
    "    \n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (b) (3 points) Plotting Bagging and Boosting\n",
    "\n",
    "Below plot the error rates (see top of the assignment) in two plots:  1 for bagging, 1 for boosting.  Plot each for 1 to 10000 rounds (plotted points should be distributed evenly on a log-scale).  Plot each for trees of depth 1, 2, and 4.  \n",
    "\n",
    "[My solutions take about 5 minutes per plot.  You may want to test with 1 to 100 or 1 to 1000 rounds first.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 57)\n",
      "(3000, 1)\n",
      "[  10   16   27   46   77  129  215  359  599 1000]\n",
      "[[-1 -1 -1 ... -1 -1 -1]\n",
      " [ 1  1  1 ...  1  1  1]\n",
      " [ 1  1  1 ...  1  1  1]\n",
      " ...\n",
      " [-1 -1 -1 ... -1 -1 -1]\n",
      " [ 1  1  1 ...  1  1  1]\n",
      " [-1 -1 -1 ... -1 -1 -1]]\n",
      "[[   1    0]\n",
      " [   1    1]\n",
      " [   1    2]\n",
      " ...\n",
      " [2996 2997]\n",
      " [2996 2998]\n",
      " [2996 2999]]\n",
      "62800.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-152754f03867>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictBoosting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtreesBoosting\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumberOfTrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mmismatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mboostingTrainErrorArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmismatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-289-f9917cbbbb9b>\u001b[0m in \u001b[0;36mpredictBoosting\u001b[0;34m(x, trees, w, numberOfTrees)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumberOfTrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtreepred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-217-fcf173eba051>\u001b[0m in \u001b[0;36mtreepred\u001b[0;34m(tree, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# value is prob of that class for that example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtreepred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# same as treepred, but returns the difference between the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "    print(trainX.shape)\n",
    "    print(trainY.shape)\n",
    "       \n",
    "    numberOfTreesArray = np.floor(np.logspace(1,3,10)).astype(int)\n",
    "    print(numberOfTreesArray)\n",
    "    depths = np.array([1,2,3])\n",
    "    \n",
    "    trainData = np.hstack((trainX,trainY))\n",
    "    \n",
    "    for depth in depths:\n",
    "        index = 1\n",
    "        baggingTrainErrorArray = np.zeros((numberOfTreesArray.size))\n",
    "        baggingTestErrorArray = np.zeros((numberOfTreesArray.size))\n",
    "        boostingTrainErrorArray = np.zeros((numberOfTreesArray.size))\n",
    "        boostingTestErrorArray = np.zeros((numberOfTreesArray.size))\n",
    "        \n",
    "        treesBagging = np.zeros(numberOfTrees)\n",
    "        treesBoosting = np.zeros(numberOfTrees)\n",
    "        w = np.zeros(numberOfTrees)\n",
    "              \n",
    "        #treesBagging = bagging(trainData,trainX,trainY,depth,numberOfTreesArray[-1])\n",
    "        #treesBoosting,w = boosting(trainData,trainX,trainY,depth,numberOfTreesArray[-1])\n",
    "        \n",
    "        for numberOfTrees in numberOfTreesArray:\n",
    "            x_train = trainX\n",
    "            y_train = trainY\n",
    "            \n",
    "            y_pred = np.zeros(trainX.shape[0])\n",
    "            \n",
    "            for i in range(numberOfTrees):\n",
    "                trees = bagging(trainData,trainX,trainY,depth,numberOfTrees)\n",
    "                y_pred = y_pred + treeout(trees,trainX)\n",
    "        \n",
    "            y_pred = y_pred/numberOfTrees\n",
    "            y_pred = np.where(y_pred >= 0, 1, -1)\n",
    "            print(y_pred)\n",
    "            \n",
    "            mismatches = np.argwhere(y_train*y_pred<0)\n",
    "            print(mismatches)\n",
    "            baggingTrainErrorArray[index] = (mismatches.shape[0]/y_train.shape[0])*100\n",
    "            \n",
    "            print(baggingTrainErrorArray[index])\n",
    "            \n",
    "            \n",
    "            y_pred = predictBoosting(x_train,treesBoosting,w,numberOfTrees)\n",
    "            mismatches = np.argwhere(sign(y_train)!=sign(y_pred))\n",
    "            boostingTrainErrorArray[index] = (length(mismatches)/length(y_train))*100\n",
    "            \n",
    "            x_test = testX\n",
    "            y_test = testY\n",
    "            \n",
    "            print(treesBagging)\n",
    "            y_pred = predictBagging(x_test,treesBagging,numberOfTrees)\n",
    "            mismatches = np.argwhere(np.sign(y_test)!=np.sign(y_pred))\n",
    "            baggingTestErrorArray[index] = (length(mismatches)/length(y_test))*100\n",
    "            \n",
    "            y_pred = predictBoosting(x_test,treesBoosting,w,numberOfTrees)\n",
    "            mismatches = np.argwhere(sign(y_test)!=sign(y_pred))\n",
    "            boostingTrainErrorArray[index] = (length(mismatches)/length(y_test))*100\n",
    "            \n",
    "            index = index + 1\n",
    "        \n",
    "        plt.semilogx(numberOfTreesArray,baggingTrainErrorArray)\n",
    "        plt.semilogx(numberOfTreesArray,baggingTestErrorArray)\n",
    "        plt.semilogx(numberOfTreesArray,boostingTrainErrorArray)\n",
    "        plt.semilogx(numberOfTreesArray,boostingTestErrorArray)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (c) (3 points) So what?\n",
    "\n",
    "What are the differences between the bagging and boosting plots and between the training and testing plots?  What do these differences say about\n",
    "(1) the choice of boosting versus bagging?  (2) the choice of the base classifier?  (3) the number of rounds to use?  (4) overfitting for these methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) There's not an outright winner; it depends on the data, the simulation and the circumstances. If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimizes the advantages and reduces pitfalls of the single model.\n",
    "\n",
    "(2) Bagging is an ensemble generation method that uses variations of samples used to train base classifiers. For each classifier to be generated, Bagging selects (with repetition) N samples from the training set with size N and train a base classifier. Boosting generates an ensemble by adding classifiers that correctly classify difficult samples.For each iteration, boosting updates the weights of the samples, so that, samples that are isclassified by the ensemble can have a higher weight, and therefore, higher probability of being selected for training the new classifier.\n",
    "\n",
    "(3)\n",
    "\n",
    "(4) If the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn't help to avoid over-fitting.\n",
    "\n",
    "In Bagging plots, we can clearly see that as we increase the depth the quality of classifier improves. Also, as we increase the number of trees the quality of classifier goes up. This is due to the fact that, where some classifier may fail to predict the correct value at a point, others might do well which improves the overall quality.\n",
    "\n",
    "Similarly in Boosting plots, increasing depth or number of trees increases the quality of the classifier. But in this case the quality of predictions is even better than the first one. This is due to the fact that individual data points have weight associated with them which helps in better prediction for individual points.\n",
    "\n",
    "Plots explains the effect of bagging and boosting on bias and variance. It is evident from the plots that bagging improves the variance whereas boosting improves bias of the underlying classifier. Detail explaination has been provided below (in the last part of  the question).\n",
    "\n",
    "Bias and Variance:\n",
    "In bagging we essentially take an average of all the classifiers generated using the bootstrap data in order to predict the output. Thus bagging doesnt affect the bias (in theory) but reduces the variance. Whereas boosting decreases the bias but increase variance which can be seen in the plots for part a and part b.\n",
    "\n",
    "In the first plot, the difference between the training error and the testing error for fixed number of trees is low which suggests that bagging improves the variance of the underlying classifier. Also, as we increase the number of trees the error rate goes down (immproves quality of prediction).\n",
    "\n",
    "In the second plot, the difference between the training error and the testing error for fixed number of trees is relatively high which suggests that boosting increases the variance. But As we increase the number of trees, the error rate quickly goes down to zero which suggest that boosting improves the bias of the underlying classifier. Also, as we increase the number of trees the error rate goes down (immproves quality of prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
